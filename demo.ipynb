{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# to use a specific GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mgtbench import AutoDetector, AutoExperiment\n",
    "from mgtbench.loading.dataloader import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "supported LLMs and detect categories:\n",
    "\n",
    "categories = ['Physics', 'Medicine', 'Biology', 'Electrical_engineering', 'Computer_science', 'Literature', 'History', 'Education', 'Art', 'Law', 'Management', 'Philosophy', 'Economy', 'Math', 'Statistics', 'Chemistry']\n",
    "\n",
    "llms = ['Moonshot', 'gpt35', 'Mixtral', 'Llama3', 'gpt-4omini']\n",
    "'''\n",
    "data_name = 'AITextDetect'\n",
    "detectLLM = 'Llama3'\n",
    "category = 'Art'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading human data\n",
      "loading machine data\n",
      "data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing data: 100%|██████████| 8394/8394 [00:00<00:00, 16179.58it/s]\n"
     ]
    }
   ],
   "source": [
    "data = load(data_name, detectLLM, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demo_data(data, size):\n",
    "    demo = {}\n",
    "    demo['train'] = {'text': data['train']['text'][:size], 'label': data['train']['label'][:size]}\n",
    "    demo['test'] = {'text': data['test']['text'][:size], 'label': data['test']['label'][:size]}\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-based Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /data1/models/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length is set to 512\n"
     ]
    }
   ],
   "source": [
    "# local path to the model, or model name on huggingface\n",
    "model_name_or_path = '/data1/models/distilbert-base-uncased'\n",
    "metric = AutoDetector.from_detector_name('LM-D', \n",
    "                                            model_name_or_path=model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate result for each data point\n",
      "Running prediction of detector LM-D\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhiyuan/miniconda3/envs/mgtbench2/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune finished\n",
      "Predict training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 140.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 161.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run classification for results\n",
      "==========\n",
      "train: Metric(acc=0.795, precision=0.7183098591549296, recall=0.9902912621359223, f1=0.8326530612244898, auc=0.9357421679511561, conf_m=None)\n",
      "test: Metric(acc=0.78, precision=0.7014925373134329, recall=0.9591836734693877, f1=0.8103448275862069, auc=0.9283713485394157, conf_m=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = AutoExperiment.from_experiment_name('supervised',detector=[metric])\n",
    "demo = get_demo_data(data, 200)\n",
    "experiment.load_data(demo)\n",
    "config = {'need_finetune': True,\n",
    "          'need_save': False,\n",
    "          'epochs': 1, # for model-based detectors\n",
    "          }\n",
    "res = experiment.launch(**config)\n",
    "\n",
    "print('==========')\n",
    "print('train:', res[0].train)\n",
    "print('test:', res[0].test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric-based Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-likelihood detector\n",
    "model_name_or_path = '/data1/zzy/gpt2-medium'\n",
    "metric = AutoDetector.from_detector_name('ll',\n",
    "                                         model_name_or_path=model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate result for each data point\n",
      "Running prediction of detector ll\n",
      "Predict training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 54.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 57.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run classification for results\n",
      "==========\n",
      "train: Metric(acc=0.835, precision=0.8301886792452831, recall=0.8543689320388349, f1=0.8421052631578947, auc=0.8934040636572915, conf_m=None)\n",
      "test: Metric(acc=0.84, precision=0.8367346938775511, recall=0.8367346938775511, f1=0.8367346938775511, auc=0.9183173269307723, conf_m=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = AutoExperiment.from_experiment_name('threshold',detector=[metric])\n",
    "demo = get_demo_data(data, 200)\n",
    "experiment.load_data(demo)\n",
    "res = experiment.launch()\n",
    "\n",
    "print('==========')\n",
    "print('train:', res[0].train)\n",
    "print('test:', res[0].test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast-DetectGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data_sda/zhiyuan/models/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "scoring_model_name_or_path = '/data_sda/zhiyuan/models/gpt-neo-2.7B'\n",
    "reference_model_name_or_path = '/data_sda/zhiyuan/models/gpt-j-6B'\n",
    "fastDetectGPT = AutoDetector.from_detector_name('fast-detectGPT', \n",
    "                                            scoring_model_name_or_path=scoring_model_name_or_path,\n",
    "                                            reference_model_name_or_path= reference_model_name_or_path\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate result for each data point\n",
      "Running prediction of detector fast-detectGPT\n",
      "Predict training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting: 100%|██████████| 100/100 [00:25<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting: 100%|██████████| 100/100 [00:26<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run classification for results\n",
      "Finding best threshold for f1 score...\n",
      "==========\n",
      "train: Metric(acc=0.89, precision=0.9069767441860465, recall=0.8478260869565217, f1=0.8764044943820225, auc=0.930756843800322, conf_m=None)\n",
      "test: Metric(acc=0.85, precision=0.8888888888888888, recall=0.8, f1=0.8421052631578947, auc=0.9292, conf_m=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = AutoExperiment.from_experiment_name('perturb', detector=[fastDetectGPT])\n",
    "demo = get_demo_data(data, 100)\n",
    "experiment.load_data(demo)\n",
    "res = experiment.launch()\n",
    "\n",
    "print('==========')\n",
    "print('train:', res[0].train)\n",
    "print('test:', res[0].test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binocolars \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data_sda/zhiyuan/models/falcon-7b /data_sda/zhiyuan/models/falcon-7b-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c96c99b407c429d937da08eaa1e9b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca7fa10eb0b402db9ffc9717b7ed9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observer_model_name_or_path = '/data_sda/zhiyuan/models/falcon-7b'\n",
    "performer_model_name_or_path = '/data_sda/zhiyuan/models/falcon-7b-instruct'\n",
    "\n",
    "binoculars = AutoDetector.from_detector_name('Binoculars', \n",
    "                                            observer_model_name_or_path=observer_model_name_or_path,\n",
    "                                            performer_model_name_or_path= performer_model_name_or_path,\n",
    "                                            max_length=1024,\n",
    "                                            mode='low-fpr', # accuracy (f1) or low-fpr\n",
    "                                            # 'default' or 'new', default is the threshold used in the paper, 'new' is the threshold calculated on the new training set\n",
    "                                            threshold='new' \n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate result for each data point\n",
      "Running prediction of detector Binoculars\n",
      "Predict training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting: 100%|██████████| 50/50 [00:16<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting: 100%|██████████| 50/50 [00:16<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run classification for results\n",
      "Finding best threshold for low-fpr...\n",
      "==========\n",
      "train: Metric(acc=0.56, precision=0.56, recall=1.0, f1=0.717948717948718, auc=0.9253246753246753, conf_m=None)\n",
      "test: Metric(acc=0.56, precision=0.56, recall=1.0, f1=0.717948717948718, auc=0.9131493506493507, conf_m=None)\n",
      "Calculate result for each data point\n",
      "Running prediction of detector Binoculars\n",
      "Predict training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting: 100%|██████████| 50/50 [00:15<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting: 100%|██████████| 50/50 [00:16<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run classification for results\n",
      "Finding best threshold for accuracy...\n",
      "==========\n",
      "train: Metric(acc=0.88, precision=0.8666666666666667, recall=0.9285714285714286, f1=0.896551724137931, auc=0.9253246753246753, conf_m=None)\n",
      "test: Metric(acc=0.86, precision=0.8620689655172413, recall=0.8928571428571429, f1=0.8771929824561403, auc=0.9131493506493507, conf_m=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# threshold set for low-fpr\n",
    "binoculars.change_mode('low-fpr')\n",
    "experiment = AutoExperiment.from_experiment_name('threshold', detector=[binoculars])\n",
    "demo = get_demo_data(data, 50)\n",
    "experiment.load_data(demo)\n",
    "res = experiment.launch()\n",
    "print('==========')\n",
    "print('train:', res[0].train)\n",
    "print('test:', res[0].test)\n",
    "\n",
    "# threshold set for f1\n",
    "binoculars.change_mode('accuracy')\n",
    "experiment = AutoExperiment.from_experiment_name('threshold', detector=[binoculars])\n",
    "experiment.load_data(demo)\n",
    "res = experiment.launch()\n",
    "print('==========')\n",
    "print('train:', res[0].train)\n",
    "print('test:', res[0].test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RADAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radar Detector is loaded\n"
     ]
    }
   ],
   "source": [
    "radar = AutoDetector.from_detector_name('RADAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate result for each data point\n",
      "Running prediction of detector RADAR\n",
      "Predict training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 53.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 52.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run classification for results\n",
      "==========\n",
      "train: Metric(acc=0.625, precision=0.7692307692307693, recall=0.3883495145631068, f1=0.5161290322580645, auc=0.7982183965569012, conf_m=None)\n",
      "test: Metric(acc=0.63, precision=0.8035714285714286, recall=0.4166666666666667, f1=0.5487804878048781, auc=0.8345410628019323, conf_m=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = AutoExperiment.from_experiment_name('supervised', detector=[radar])\n",
    "demo = get_demo_data(data, 200)\n",
    "experiment.load_data(demo)\n",
    "res = experiment.launch()\n",
    "\n",
    "print('==========')\n",
    "print('train:', res[0].train)\n",
    "print('test:', res[0].test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7260251f4d2b4f4ca6a1ec2116d219a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = '/data1/models/Llama-2-7b-chat-hf'\n",
    "mask_model_name_or_path = '/data1/models/t5-base'\n",
    "detectGPT = AutoDetector.from_detector_name('detectGPT', \n",
    "                                            model_name_or_path=model_name_or_path,\n",
    "                                            mask_model_name_or_path= mask_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate result for each data point\n",
      "Running prediction of detector detectGPT\n",
      "Predict training data\n",
      "Running perturb on the given texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:47<00:00, 15.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturb finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.41it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict testing data\n",
      "Running perturb on the given texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:52<00:00, 17.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturb finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.60it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run classification for results\n",
      "==========\n",
      "train: Metric(acc=0.6, precision=0.5, recall=0.75, f1=0.6, auc=0.6458333333333334, conf_m=None)\n",
      "test: Metric(acc=0.8, precision=0.8, recall=0.8, f1=0.8, auc=0.7200000000000002, conf_m=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = AutoExperiment.from_experiment_name('perturb',detector=[detectGPT])\n",
    "demo = get_demo_data(data, 10)\n",
    "experiment.load_data(demo)\n",
    "res = experiment.launch(n_perturbations=5)\n",
    "\n",
    "print('==========')\n",
    "print('train:', res[0].train)\n",
    "print('test:', res[0].test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNA-gpt Detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name_or_path = '/data1/zzy/gpt2-medium'\n",
    "dna_gpt = AutoDetector.from_detector_name('DNA-GPT',\n",
    "                                          base_model_name_or_path=base_model_name_or_path,\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate result for each data point\n",
      "Running prediction of detector DNA-GPT\n",
      "Predict training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting: 100%|██████████| 5/5 [00:48<00:00,  9.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting: 100%|██████████| 5/5 [00:42<00:00,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run classification for results\n",
      "Finding best threshold for accuracy...\n",
      "==========\n",
      "train: Metric(acc=0.8, precision=1.0, recall=0.5, f1=0.6666666666666666, auc=0.6666666666666667, conf_m=None)\n",
      "test: Metric(acc=0.8, precision=0.0, recall=0.0, f1=0.0, auc=0.75, conf_m=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/zhiyuan/miniconda3/envs/mgtbench2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "experiment = AutoExperiment.from_experiment_name('perturb', detector=[dna_gpt])\n",
    "demo = get_demo_data(data, 5)\n",
    "experiment.load_data(demo)\n",
    "res = experiment.launch()\n",
    "\n",
    "print('==========')\n",
    "print('train:', res[0].train)\n",
    "print('test:', res[0].test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many-shot settings\n",
    "Given many shot data, we are able to fine-tune the detector, which is equalevent to class-incremental learning.\n",
    "\n",
    "In each stage, the model is fine-tuned on only the data in current stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_incremental_topic can be used by formatting the order\n",
    "\n",
    "---\n",
    "```python\n",
    "# two stages. first stage includes 4 classes (with human) and the second stage incude 2 classes\n",
    "order = [['gpt35', 'Mixtral','Moonshot',], ['Llama3','gpt-4omini']]\n",
    "```\n",
    "---\n",
    "```python\n",
    "# fives stages. first stage includes 2 classes (with human) and each remaining stage includes 1 class\n",
    "order = [['Moonshot'],['Mixtral'],['gpt35'],['Llama3'],['gpt-4omini']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [['Moonshot'],['Mixtral'],['gpt35'],['Llama3'],['gpt-4omini']]\n",
    "order = [['gpt35', 'Mixtral','Moonshot','Llama3'],['gpt-4omini']]\n",
    "from mgtbench.loading import load_incremental_topic, load_incremental\n",
    "\n",
    "data = load_incremental_topic(order, \"Social_sciences\")\n",
    "# data = load_incremental(order, \"Art\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0, 1, 2, 3, 4}, {5})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data['train'][0]['label']), set(data['train'][1]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nclass = len(set(data['train'][0]['label']))\n",
    "nclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mgtbench import AutoDetector, AutoExperiment\n",
    "method = 'incremental'\n",
    "model= '/data1/models/roberta-base'\n",
    "nclass = len(set(data['train'][0]['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based Incremental Tenchniques\n",
    "We implement LwF, iCaRL, BiC for adapatation, which are controlled by parameters:\n",
    "\n",
    "`lwf_reg` is a distillation-based regularization parameter for LwF\n",
    "\n",
    "`cache_size` is for iCaRL. Based on the class_mean, select the top-k closest samples in each class for cache\n",
    "\n",
    "`bic` is for BiC layer. It calibrates the output logits for each class\n",
    "\n",
    "Additionally, `factor` is decayed lr. For example, learning rate is initialized to lr in stage1, and rate is initialized to lr/factor in stage2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m metric \u001b[38;5;241m=\u001b[39m AutoDetector\u001b[38;5;241m.\u001b[39mfrom_detector_name(method, model_name_or_path\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m----> 2\u001b[0m                                         num_labels\u001b[38;5;241m=\u001b[39mnclass, lwf_reg \u001b[38;5;241m=\u001b[39m \u001b[43mreg\u001b[49m,cache_size\u001b[38;5;241m=\u001b[39msize,bic\u001b[38;5;241m=\u001b[39mbic)\n\u001b[1;32m      3\u001b[0m experiment \u001b[38;5;241m=\u001b[39m AutoExperiment\u001b[38;5;241m.\u001b[39mfrom_experiment_name(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincremental\u001b[39m\u001b[38;5;124m'\u001b[39m,detector\u001b[38;5;241m=\u001b[39m[metric])\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed_finetune\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed_save\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_factor\u001b[39m\u001b[38;5;124m'\u001b[39m: factor\n\u001b[1;32m     12\u001b[0m         }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reg' is not defined"
     ]
    }
   ],
   "source": [
    "metric = AutoDetector.from_detector_name(method, model_name_or_path=model,\n",
    "                                        num_labels=nclass, lwf_reg = reg,cache_size=size,bic=bic)\n",
    "experiment = AutoExperiment.from_experiment_name('incremental',detector=[metric])\n",
    "config = {'need_finetune': True,\n",
    "        'need_save': False,\n",
    "        'epochs': 2,\n",
    "        'lr': 1e-6,\n",
    "        'batch_size':64,\n",
    "        'save_path': '/data1/lyl/mgtout-1/',\n",
    "        'eval':True,\n",
    "        'lr_factor': factor\n",
    "        }\n",
    "# cfg = SupervisedConfig()\n",
    "# cfg.update(config)\n",
    "experiment.load_data(data)\n",
    "res1 = experiment.launch(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mgtbench import AutoDetector, AutoExperiment\n",
    "method = 'incremental'\n",
    "model= '/data1/models/roberta-base'\n",
    "nclass = len(set(data['train'][0]['label']))\n",
    "\n",
    "scoring_model_name_or_path = '/data_sda/zhiyuan/models/gpt-neo-2.7B'\n",
    "reference_model_name_or_path = '/data_sda/zhiyuan/models/gpt-j-6B'\n",
    "detector = AutoDetector.from_detector_name('fast-detectGPT', \n",
    "                                            scoring_model_name_or_path=scoring_model_name_or_path,\n",
    "                                            reference_model_name_or_path= reference_model_name_or_path\n",
    "                                            )\n",
    "experiment = AutoExperiment.from_experiment_name('incremental_threshold',detector=[detector],cache_size=0)\n",
    "experiment.load_data(data)\n",
    "experiment.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
